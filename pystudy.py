import streamlit as st
import google.generativeai as genai
import os
import time
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import asyncio

# --- âš™ï¸ Gemini API í‚¤ ì„¤ì • (ìµœì í™”) ---
@st.cache_resource
def configure_gemini():
    """API ì„¤ì •ì„ ìºì‹œí•˜ì—¬ ì¬ì´ˆê¸°í™” ë°©ì§€"""
    try:
        api_key = st.secrets.get("GEMINI_API_KEY") or os.environ.get("GEMINI_API_KEY")
        if not api_key:
            st.error("ğŸ”‘ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            st.stop()
        
        genai.configure(api_key=api_key)
        
        # ì„±ëŠ¥ ìµœì í™”ëœ ì„¤ì •
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            max_output_tokens=1024,  # í† í° ìˆ˜ ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ
            top_p=0.8,
            top_k=20  # ì¤„ì¸ ê°’ìœ¼ë¡œ ì†ë„ ê°œì„ 
        )
        
        model = genai.GenerativeModel(
            'gemini-flash',
            generation_config=generation_config
        )
        return model
    except Exception as e:
        st.error(f"âŒ API ì„¤ì • ì‹¤íŒ¨: {e}")
        st.stop()

# íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” AI ì‘ë‹µ í•¨ìˆ˜
def get_ai_response_with_timeout(model, prompt, timeout=15):
    """íƒ€ì„ì•„ì›ƒì„ ì ìš©í•œ AI ì‘ë‹µ ìƒì„±"""
    def generate_response():
        return model.generate_content(prompt)
    
    with ThreadPoolExecutor() as executor:
        future = executor.submit(generate_response)
        try:
            response = future.result(timeout=timeout)
            return response.text
        except TimeoutError:
            return "â° ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ê°„ë‹¨í•œ ìš”ì²­ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# --- ğŸš€ í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="íŒŒì´ì¬ ì½”ë”© í•™ìŠµ & ë¬¸ì œí’€ì´",
    page_icon="ğŸ’¡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ëª¨ë¸ ì´ˆê¸°í™”
model = configure_gemini()

st.title("ğŸ“š íŒŒì´ì¬ ì½”ë”© í•™ìŠµ & ë¬¸ì œí’€ì´ ğŸ’¡")
st.caption("âš¡ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ AI ê¸°ë°˜ í•™ìŠµ ë„êµ¬")

# --- ğŸ§­ ì‚¬ì´ë“œë°” ë©”ë‰´ ---
st.sidebar.header("ğŸ¯ ë©”ë‰´")
menu_choice = st.sidebar.radio(
    "ì›í•˜ëŠ” ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
    ('ğŸ“– í•™ìŠµí•˜ê¸°', 'ğŸ“ ë¬¸ì œí’€ì´', 'â“ Q&A'),
    help="ê° ë©”ë‰´ë¥¼ í´ë¦­í•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”!"
)

# ì‚¬ì´ë“œë°”ì— ì„±ëŠ¥ íŒ ì¶”ê°€
st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ’¡ ì„±ëŠ¥ íŒ")
st.sidebar.info(
    "â€¢ ê°„ë‹¨í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë¹ ë¥¸ ì‘ë‹µ\n"
    "â€¢ ë¬¸í•­ ìˆ˜ë¥¼ ì ê²Œ ì„¤ì •í•˜ë©´ ì†ë„ í–¥ìƒ\n"
    "â€¢ ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸ í•„ìš”"
)

# ### ğŸ“– í•™ìŠµí•˜ê¸°
if menu_choice == 'ğŸ“– í•™ìŠµí•˜ê¸°':
    st.header("ğŸ“– í•™ìŠµí•˜ê¸°")
    st.write("ê´€ì‹¬ ìˆëŠ” ì»¤ë¦¬í˜ëŸ¼ì„ ì„ íƒí•˜ê³  íŒŒì´ì¬ í•™ìŠµ ë‚´ìš©ì„ ë°›ì•„ë³´ì„¸ìš”!")

    # ë¯¸ë¦¬ ì •ì˜ëœ ì»¤ë¦¬í˜ëŸ¼ (DB ëŒ€ì‹  í•˜ë“œì½”ë”©ìœ¼ë¡œ ì†ë„ í–¥ìƒ)
    curriculums = {
        "ğŸ íŒŒì´ì¬ ê¸°ì´ˆ": "ë³€ìˆ˜, ìë£Œí˜•, ì—°ì‚°ì",
        "âš™ï¸ ì œì–´ë¬¸": "ifë¬¸, forë¬¸, whileë¬¸",
        "âœï¸ í•¨ìˆ˜": "í•¨ìˆ˜ ì •ì˜, ë§¤ê°œë³€ìˆ˜, ë°˜í™˜ê°’",
        "ğŸ§± í´ë˜ìŠ¤": "í´ë˜ìŠ¤, ê°ì²´, ìƒì†",
        "ğŸ“Š ë°ì´í„°ë¶„ì„": "Numpy, Pandas ê¸°ì´ˆ",
        "ğŸŒ ì›¹ê°œë°œ": "Flask, Django ì…ë¬¸"
    }
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        selected_curriculum = st.selectbox(
            "í•™ìŠµ ì»¤ë¦¬í˜ëŸ¼ ì„ íƒ:", 
            list(curriculums.keys()),
            help="í•™ìŠµí•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”"
        )
    
    with col2:
        study_level = st.selectbox("ë‚œì´ë„:", ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"])

    if st.button("âœ¨ í•™ìŠµ ë‚´ìš© ìƒì„±", type="primary"):
        if selected_curriculum:
            # ì»´íŒ©íŠ¸í•œ ì§„í–‰ í‘œì‹œ
            with st.status("AIê°€ í•™ìŠµ ë‚´ìš©ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...", expanded=True) as status:
                st.write("ğŸ“ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
                
                # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ë¡œ ìµœì í™”
                curriculum_detail = curriculums[selected_curriculum]
                prompt = f"""
                íŒŒì´ì¬ {curriculum_detail} ({study_level})ì— ëŒ€í•´:
                1. í•µì‹¬ ê°œë… 2ê°€ì§€
                2. ê° ê°œë…ë³„ ê°„ë‹¨í•œ ì½”ë“œ ì˜ˆì œ 1ê°œì”©
                3. ì‹¤ìŠµ íŒ 1ê°€ì§€
                
                ì´ 500ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                """
                
                start_time = time.time()
                response_text = get_ai_response_with_timeout(model, prompt, timeout=10)
                generation_time = time.time() - start_time
                
                status.update(label="âœ… ìƒì„± ì™„ë£Œ!", state="complete")
                st.write(f"â±ï¸ ìƒì„± ì‹œê°„: {generation_time:.1f}ì´ˆ")
            
            # ê²°ê³¼ í‘œì‹œ
            st.markdown("### ğŸ“š í•™ìŠµ ë‚´ìš©")
            if "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in response_text or "ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤" in response_text:
                st.error(response_text)
                st.info("ğŸ’¡ **í•´ê²° ë°©ë²•**: ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë” ê°„ë‹¨í•œ ì£¼ì œë¥¼ ì„ íƒí•´ë³´ì„¸ìš”.")
            else:
                st.markdown(response_text)
                st.success("ğŸ‰ í•™ìŠµ ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.warning("âš ï¸ í•™ìŠµí•  ì»¤ë¦¬í˜ëŸ¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")

# ### ğŸ“ ë¬¸ì œí’€ì´
elif menu_choice == 'ğŸ“ ë¬¸ì œí’€ì´':
    st.header("ğŸ“ ë¬¸ì œí’€ì´")
    st.write("ì›í•˜ëŠ” ë¬¸ì œ ìœ í˜•ì„ ì„ íƒí•˜ì—¬ íŒŒì´ì¬ ì½”ë”© ë¬¸ì œë¥¼ í’€ì–´ë³´ì„¸ìš”!")

    # ë¯¸ë¦¬ ì •ì˜ëœ ë¬¸ì œ ë²”ìœ„ (ì„±ëŠ¥ ìµœì í™”)
    problem_ranges = [
        "ë³€ìˆ˜ì™€ ìë£Œí˜•", "ì¡°ê±´ë¬¸", "ë°˜ë³µë¬¸", "í•¨ìˆ˜", 
        "ë¦¬ìŠ¤íŠ¸", "ë”•ì…”ë„ˆë¦¬", "í´ë˜ìŠ¤", "ì˜ˆì™¸ì²˜ë¦¬"
    ]
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        num_questions = st.selectbox(
            "ë¬¸í•­ ìˆ˜:", 
            [1, 2, 3], 
            help="ë§ì€ ë¬¸í•­ì¼ìˆ˜ë¡ ìƒì„± ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤"
        )
    
    with col2:
        selected_range = st.selectbox("ë¬¸ì œ ë²”ìœ„:", problem_ranges)
    
    with col3:
        question_type = st.selectbox(
            "ë¬¸ì œ ìœ í˜•:",
            ['ê°ê´€ì‹', 'ë¹ˆì¹¸ì±„ìš°ê¸°', 'ë‹¨ë‹µì‹']
        )

    if st.button("ğŸš€ ë¬¸ì œ ìƒì„±", type="primary"):
        if selected_range:
            with st.status("ë¬¸ì œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...", expanded=True) as status:
                st.write("ğŸ§  AIê°€ ë¬¸ì œë¥¼ ë§Œë“¤ê³  ìˆì–´ìš”...")
                
                # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
                type_map = {
                    'ê°ê´€ì‹': '4ì§€ì„ ë‹¤ ë¬¸ì œ',
                    'ë¹ˆì¹¸ì±„ìš°ê¸°': 'ì½”ë“œ ë¹ˆì¹¸ ë¬¸ì œ', 
                    'ë‹¨ë‹µì‹': 'ìš©ì–´ ë‹¨ë‹µ ë¬¸ì œ'
                }
                
                prompt = f"""
                íŒŒì´ì¬ {selected_range} ê´€ë ¨ {type_map[question_type]} {num_questions}ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
                ê° ë¬¸ì œë§ˆë‹¤ ì •ë‹µê³¼ ê°„ë‹¨í•œ í•´ì„¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
                ì´ ê¸¸ì´ëŠ” 600ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                """
                
                start_time = time.time()
                response_text = get_ai_response_with_timeout(model, prompt, timeout=12)
                generation_time = time.time() - start_time
                
                status.update(label="âœ… ë¬¸ì œ ìƒì„± ì™„ë£Œ!", state="complete")
                st.write(f"â±ï¸ ìƒì„± ì‹œê°„: {generation_time:.1f}ì´ˆ")
            
            # ë¬¸ì œ í‘œì‹œ
            st.markdown("### ğŸ“‹ ìƒì„±ëœ ë¬¸ì œ")
            if "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in response_text or "ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤" in response_text:
                st.error(response_text)
                st.info("ğŸ’¡ **í•´ê²° ë°©ë²•**: ë¬¸í•­ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
            else:
                st.markdown(response_text)
                st.balloons()  # ì„±ê³µ ì‹œ ì¶•í•˜ íš¨ê³¼
        else:
            st.warning("âš ï¸ ë¬¸ì œ ë²”ìœ„ë¥¼ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")

# ### â“ Q&A
elif menu_choice == 'â“ Q&A':
    st.header("â“ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”!")
    st.write("íŒŒì´ì¬ ì½”ë”©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¤”")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ (ìµœëŒ€ 10ê°œë¡œ ì œí•œí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ)
    recent_history = st.session_state.chat_history[-10:] if len(st.session_state.chat_history) > 10 else st.session_state.chat_history
    
    for message in recent_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì±„íŒ… ì…ë ¥
    user_question = st.chat_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”... (ì˜ˆ: íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë€ ë¬´ì—‡ì¸ê°€ìš”?)")

    if user_question:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        # AI ì‘ë‹µ
        with st.chat_message("assistant"):
            with st.status("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...", expanded=False) as status:
                # ê°„ê²°í•œ Q&A í”„ë¡¬í”„íŠ¸
                prompt = f"""
                íŒŒì´ì¬ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                ì§ˆë¬¸: {user_question}
                
                ë‹µë³€ ê¸¸ì´: 300ì ì´ë‚´
                í¬í•¨ì‚¬í•­: í•µì‹¬ ì„¤ëª…, ê°„ë‹¨í•œ ì˜ˆì œ(í•„ìš”ì‹œ)
                """
                
                start_time = time.time()
                response_text = get_ai_response_with_timeout(model, prompt, timeout=8)
                response_time = time.time() - start_time
                
                status.update(label=f"âœ… ë‹µë³€ ì™„ë£Œ! ({response_time:.1f}ì´ˆ)", state="complete")
            
            if "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in response_text or "ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤" in response_text:
                st.error(response_text)
                st.info("ğŸ’¡ ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
                # ì˜¤ë¥˜ ì‹œ ì‚¬ìš©ì ì§ˆë¬¸ ì œê±°
                if st.session_state.chat_history[-1]["role"] == "user":
                    st.session_state.chat_history.pop()
            else:
                st.markdown(response_text)
                # AI ì‘ë‹µ ì €ì¥
                st.session_state.chat_history.append({"role": "assistant", "content": response_text})

    # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ë²„íŠ¼
    if st.session_state.chat_history:
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"):
                st.session_state.chat_history = []
                st.rerun()
        with col2:
            st.write(f"ğŸ’¬ ì´ ëŒ€í™”: {len(st.session_state.chat_history)}ê°œ")

# --- í‘¸í„° ---
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
    âš¡ ìµœì í™”ëœ AI í•™ìŠµ ë„êµ¬ | ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ì„¤ê³„ë¨
    </div>
    """, 
    unsafe_allow_html=True
)
